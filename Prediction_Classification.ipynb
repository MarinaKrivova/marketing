{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive model - Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, optimisation of marketing campaign is made by training machine learning model and analysis of features estimated to be the most responsible for the final predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing\n",
    "1. Filling missing values\n",
    "\n",
    "2. Resolving correlation between euribor3m and emp.var.rate variables??? - Try later\n",
    "\n",
    "In the original work of S. Moro et al, both variables were used. Moreover, both of them appeared to be among top8 most important features (euribor3m on the 1st place, and emp.var.rate on the 8th)\n",
    "\n",
    "3. Transformation of numerical variables into categorical ones??? - Try later\n",
    "4. Encoding categorical varibles/transforming into dumnies columns\n",
    "5. Resampling for train/test splits due to unbalanced targets\n",
    "\n",
    "#### ML training and testing\n",
    "In the original work published in 2014, the best results were obtained with NN ensemble composed of Nr = 7 distinct networks, each trained with 100 epochs of the BFGS algorithm. The number of hidden nodes was H = round(M/2) (M is the number of inputs). Among other tested models there were LogisticRegression, SVM and RandomForest.\n",
    "\n",
    "From the view point of results interpretibility, most promissing current alternatives are boosted models, particularly XGBoost, CatBoost and LightGBM classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://archive.ics.uci.edu/ml/datasets/Bank+Marketing\n",
    "    \n",
    "**Data Set Information:**\n",
    "\n",
    "The data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed.\n",
    "\n",
    "There are four datasets:\n",
    "\n",
    " 1. bank-additional-full.csv with all examples (41188) and 20 inputs, ordered by date (from May 2008 to November 2010), very close to the data analyzed in [Moro et al., 2014]\n",
    " 2. bank-additional.csv with 10% of the examples (4119), randomly selected from 1), and 20 inputs.\n",
    " 3. bank-full.csv with all examples and 17 inputs, ordered by date (older version of this dataset with less inputs).\n",
    " 4. bank.csv with 10% of the examples and 17 inputs, randomly selected from 3 (older version of this dataset with less inputs).\n",
    " \n",
    "The smallest datasets are provided to test more computationally demanding machine learning algorithms (e.g., SVM).\n",
    "\n",
    "The classification goal is to predict if the client will subscribe (yes/no) a term deposit (variable y).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attribute Information:\n",
    "\n",
    "Input variables:\n",
    "#### bank client data:\n",
    "\n",
    "1.  **age** (numeric)\n",
    "2.  **job** : type of job (categorical: 'admin.','bluecollar','entrepreneur','housemaid','management','retired','selfemployed','services','student','technician','unemployed','unknown')\n",
    "3.  **marital** : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n",
    "4.  **education** (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')\n",
    "5.  **default**: has credit in default? (categorical: 'no','yes','unknown')\n",
    "6.  **housing**: has housing loan? (categorical: 'no','yes','unknown')\n",
    "7.  **loan**: has personal loan? (categorical: 'no','yes','unknown')\n",
    "\n",
    "#### related with the last contact of the current campaign:\n",
    "8.  **contact**: contact communication type (categorical: 'cellular','telephone')\n",
    "9.  **month**: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n",
    "10.  **day_of_week**: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')\n",
    "11.  **duration**: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n",
    "\n",
    "#### other attributes:\n",
    "12.  **campaign**: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n",
    "13.  **pdays**: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n",
    "14.  **previous**: number of contacts performed before this campaign and for this client (numeric)\n",
    "15.  **poutcome**: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')\n",
    "\n",
    "#### social and economic context attributes\n",
    "16.  **emp.var.rate**: employment variation rate  quarterly indicator (numeric)\n",
    "17.  **cons.price.idx**: consumer price index  monthly indicator (numeric)\n",
    "18.  **cons.conf.idx**: consumer confidence index  monthly indicator (numeric)\n",
    "19.  **euribor3m**: euribor 3 month rate  daily indicator (numeric)\n",
    "20.  **nr.employed**: number of employees  quarterly indicator (numeric)\n",
    "\n",
    "Output variable (desired target):\n",
    "21.  **y**  has the client subscribed a term deposit? (binary: 'yes','no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the libraries\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import random\n",
    "random.seed(123)\n",
    "\n",
    "from IPython.display import display\n",
    "import random\n",
    "import time\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "import os\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr.employed</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56</td>\n",
       "      <td>housemaid</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.4y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>261</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>149</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>226</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.6y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>151</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>307</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age        job  marital    education  default housing loan    contact  \\\n",
       "0   56  housemaid  married     basic.4y       no      no   no  telephone   \n",
       "1   57   services  married  high.school  unknown      no   no  telephone   \n",
       "2   37   services  married  high.school       no     yes   no  telephone   \n",
       "3   40     admin.  married     basic.6y       no      no   no  telephone   \n",
       "4   56   services  married  high.school       no      no  yes  telephone   \n",
       "\n",
       "  month day_of_week  duration  campaign  pdays  previous     poutcome  \\\n",
       "0   may         mon       261         1    999         0  nonexistent   \n",
       "1   may         mon       149         1    999         0  nonexistent   \n",
       "2   may         mon       226         1    999         0  nonexistent   \n",
       "3   may         mon       151         1    999         0  nonexistent   \n",
       "4   may         mon       307         1    999         0  nonexistent   \n",
       "\n",
       "   emp.var.rate  cons.price.idx  cons.conf.idx  euribor3m  nr.employed   y  \n",
       "0           1.1          93.994          -36.4      4.857       5191.0  no  \n",
       "1           1.1          93.994          -36.4      4.857       5191.0  no  \n",
       "2           1.1          93.994          -36.4      4.857       5191.0  no  \n",
       "3           1.1          93.994          -36.4      4.857       5191.0  no  \n",
       "4           1.1          93.994          -36.4      4.857       5191.0  no  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"bank-additional/bank-additional-full.csv\", sep =\";\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_dict= {\"no\":0, \"yes\":1}\n",
    "df[\"y\"] = df[\"y\"].map(map_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cat_columns = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <th>default</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">0</th>\n",
       "      <th>no</th>\n",
       "      <td>28391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unknown</th>\n",
       "      <td>8154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yes</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>no</th>\n",
       "      <td>4197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unknown</th>\n",
       "      <td>443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             age\n",
       "y default       \n",
       "0 no       28391\n",
       "  unknown   8154\n",
       "  yes          3\n",
       "1 no        4197\n",
       "  unknown    443"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby([\"y\",\"default\"])[[\"age\"]].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most reasonable way here would be fill with \"no\" values, but in that case we need to drop the entire column, as it will have only \"no\" values. \n",
    "\n",
    "So it will be filled randomly with \"yes\" and \"no\" values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: job, Number of unique values: 12\n",
      "Column: marital, Number of unique values: 4\n",
      "Column: education, Number of unique values: 8\n",
      "Column: default, Number of unique values: 3\n",
      "Column: housing, Number of unique values: 3\n",
      "Column: loan, Number of unique values: 3\n",
      "Column: contact, Number of unique values: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'job': ['blue-collar',\n",
       "  'management',\n",
       "  'self-employed',\n",
       "  'student',\n",
       "  'retired',\n",
       "  'unemployed',\n",
       "  'housemaid',\n",
       "  'entrepreneur',\n",
       "  'admin.',\n",
       "  'technician',\n",
       "  'services'],\n",
       " 'marital': ['single', 'married', 'divorced'],\n",
       " 'education': ['basic.9y',\n",
       "  'high.school',\n",
       "  'illiterate',\n",
       "  'basic.4y',\n",
       "  'university.degree',\n",
       "  'professional.course',\n",
       "  'basic.6y'],\n",
       " 'default': ['no', 'yes'],\n",
       " 'housing': ['no', 'yes'],\n",
       " 'loan': ['no', 'yes'],\n",
       " 'contact': ['cellular', 'telephone']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_values_dict = {}\n",
    "for col in text_cat_columns:\n",
    "    print(\"Column: %s, Number of unique values: %d\"% (col, df[col].nunique()))\n",
    "    cat_values_dict[col] = list(set(df[col].unique())-set([\"unknown\"]))\n",
    "\n",
    "cat_values_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job 330\n",
      "marital 80\n",
      "education 1731\n",
      "default 8597\n",
      "housing 990\n",
      "loan 990\n",
      "contact 0\n"
     ]
    }
   ],
   "source": [
    "# filling with random values from lists of availbale categories\n",
    "\n",
    "for col in text_cat_columns:\n",
    "    n_missing = df[df[col]==\"unknown\"].shape[0]\n",
    "    missing_index = df[df[col]==\"unknown\"].index\n",
    "    print(col, n_missing)\n",
    "    fill_vallues = cat_values_dict[col]*n_missing\n",
    "    random.shuffle(fill_vallues)\n",
    "    df.loc[missing_index, col]= fill_vallues[:n_missing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <th>default</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>no</th>\n",
       "      <td>32441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yes</th>\n",
       "      <td>4107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>no</th>\n",
       "      <td>4433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yes</th>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             age\n",
       "y default       \n",
       "0 no       32441\n",
       "  yes       4107\n",
       "1 no        4433\n",
       "  yes        207"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby([\"y\",\"default\"])[[\"age\"]].count()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "most_freq_dict ={}\n",
    "for col in text_cat_columns:\n",
    "    gr = df.groupby([\"y\", col])[[\"age\"]].count()\n",
    "    y_0 = gr.reset_index(col).loc[0, col].values[0]\n",
    "    y_1 = gr.reset_index(col).loc[1, col].values[0]\n",
    "    if y_0 ==y_1:\n",
    "        most_freq_dict[col] = y_0\n",
    "    else:\n",
    "        print(\"Not equal data for column %s\" % col)\n",
    "most_freq_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding categories & Correlated predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible scenarios:\n",
    "1. leave correlated predictors (as it was done in the original work) and create dummies columns for all categorical variables\n",
    "2. leave correlated predictors (as it was done in the original work) and make OHE/Label encoding\n",
    "3. drop one of the correlated features and make OHE/Label encoding\n",
    "\n",
    "Considering our final task on giving recommendations on optimisation, the first scenario seem to provide more valuable information. Therefore first approach is tested in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OHE in one column\n",
    "df2 = df.copy()\n",
    "map_yes_no_dict = {\"yes\": 1, \"no\":0}\n",
    "map_phone_dict = {\"cellular\": 1, \"telephone\":0}\n",
    "for col in ['default', 'housing','loan']:\n",
    "    df2[col] = df2[col].map(map_yes_no_dict)\n",
    "df2['contact'] = df2['contact'].map(map_phone_dict) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data: (41188, 21), Encoded data: (41188, 21)\n"
     ]
    }
   ],
   "source": [
    "# for all the rest - dummies columns\n",
    "df1 = pd.get_dummies(df)\n",
    "print(\"Original data: {}, Encoded data: {}\".format(df.shape, df2.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of features increased by 33 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets\n",
    "\n",
    "For separating a train set  it is more reasonable to use resampling\n",
    "which means removing samples from the majority class (under-sampling) and \n",
    "adding more examples from the minority class (over-sampling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36548, 4640, 7.88)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_0, y_1 = df[\"y\"].value_counts()\n",
    "y_0, y_1, round(y_0/y_1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0 = df[df[\"y\"] == 0]\n",
    "df_1 = df[df[\"y\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df1.drop(\"y\", axis=1)\n",
    "y = df1[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((26360, 57), (26360,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify= y)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, stratify= y_train)\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original proportion of classes: 7.88\n",
      "Proportion in test data after splitting: 8.88\n"
     ]
    }
   ],
   "source": [
    "print(\"Original proportion of classes:\", round(df1[df1[\"y\"]==0].shape[0]/df1[df1[\"y\"]==1].shape[0], 2)) \n",
    "print(\"Proportion in test data after splitting:\", round(len(y_test)/np.sum(y_test[y_test ==1]), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1. Resampling"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Undersampling for the majority class\n",
    "#\"Tomek links are pairs of very close instances, but of opposite classes. \n",
    "# Removing the instances of the majority class of each pair increases the space between the two classes, \n",
    "# facilitating the classification process.\"\n",
    "\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "# tomek_links = TomekLinks(sampling_strategy='majority')\n",
    "tomek_links = TomekLinks()\n",
    "\n",
    "# fit the object to the training data.\n",
    "X_train_tl, y_train_tl = tomek_links.fit_sample(X_train, y_train)\n",
    "\n",
    "len(y_train_tl[y_train_tl==0]), len(y_train_tl[y_train_tl==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(23188, 23188, 26360)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SMOTE (Synthetic Minority Oversampling TEchnique) consists of synthesizing elements for the minority class, \n",
    "# based on those that already exist. It works randomly picingk a point from the minority class \n",
    "# and computing the k-nearest neighbors for this point. \n",
    "# The synthetic points are added between the chosen point and its neighbors\n",
    "\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "smotemek = SMOTETomek(sampling_strategy='auto')\n",
    "\n",
    "# fit the object to our training data.\n",
    "X_train2, y_train2 = smotemek.fit_sample(X_train, y_train)\n",
    "\n",
    "len(y_train2[y_train2==0]), len(y_train2[y_train2==1]), len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.fit_transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_models_results(my_cv_results):\n",
    "    print(\"Best Parameters: {}\\n\".format(my_cv_results.best_params_))\n",
    "    means = my_cv_results.cv_results_[\"mean_test_score\"]\n",
    "    stds = my_cv_results.cv_results_[\"std_test_score\"]\n",
    "    for mean, std, params in zip(means, stds, my_cv_results.cv_results_[\"params\"]):\n",
    "        print(\"{} +/- {} for {}\".format(round(mean, 3), round(std*2, 3), params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 2, 'n_estimators': 5}\n",
      "\n",
      "0.882 +/- 0.019 for {'max_depth': 2, 'n_estimators': 5}\n",
      "0.839 +/- 0.173 for {'max_depth': 2, 'n_estimators': 50}\n",
      "0.841 +/- 0.161 for {'max_depth': 2, 'n_estimators': 250}\n",
      "0.662 +/- 0.583 for {'max_depth': 4, 'n_estimators': 5}\n",
      "0.655 +/- 0.432 for {'max_depth': 4, 'n_estimators': 50}\n",
      "0.673 +/- 0.573 for {'max_depth': 4, 'n_estimators': 250}\n",
      "0.498 +/- 0.649 for {'max_depth': 8, 'n_estimators': 5}\n",
      "0.496 +/- 0.643 for {'max_depth': 8, 'n_estimators': 50}\n",
      "0.492 +/- 0.635 for {'max_depth': 8, 'n_estimators': 250}\n",
      "0.457 +/- 0.615 for {'max_depth': 16, 'n_estimators': 5}\n",
      "0.442 +/- 0.585 for {'max_depth': 16, 'n_estimators': 50}\n",
      "0.455 +/- 0.613 for {'max_depth': 16, 'n_estimators': 250}\n",
      "0.451 +/- 0.609 for {'max_depth': None, 'n_estimators': 5}\n",
      "0.45 +/- 0.594 for {'max_depth': None, 'n_estimators': 50}\n",
      "0.44 +/- 0.582 for {'max_depth': None, 'n_estimators': 250}\n",
      "CPU times: user 2min 50s, sys: 4 s, total: 2min 54s\n",
      "Wall time: 3min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rf = RandomForestClassifier()\n",
    "parameters = {\"n_estimators\" : [5, 50, 250],\n",
    "             \"max_depth\": [2,4,8,16, None]}\n",
    "\n",
    "rf_cv = GridSearchCV(rf, parameters, cv=5) #by defult refit =True, so the best model is automatically retrained on the whole data\n",
    "rf_cv.fit(X, y.ravel())\n",
    "\n",
    "print_models_results(rf_cv)\n",
    "rf_best = rf_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 5}\n",
      "\n",
      "0.887 +/- 0.0 for {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 5}\n",
      "0.887 +/- 0.0 for {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 50}\n",
      "0.73 +/- 0.587 for {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 250}\n",
      "0.887 +/- 0.0 for {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 5}\n",
      "0.887 +/- 0.0 for {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50}\n",
      "0.653 +/- 0.555 for {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 250}\n",
      "0.887 +/- 0.0 for {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 5}\n",
      "0.887 +/- 0.0 for {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 50}\n",
      "0.483 +/- 0.637 for {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 250}\n",
      "0.887 +/- 0.0 for {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 5}\n",
      "0.887 +/- 0.0 for {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 50}\n",
      "0.461 +/- 0.585 for {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 250}\n",
      "0.739 +/- 0.594 for {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 5}\n",
      "0.689 +/- 0.58 for {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 50}\n",
      "0.663 +/- 0.558 for {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 250}\n",
      "0.691 +/- 0.577 for {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 5}\n",
      "0.553 +/- 0.564 for {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50}\n",
      "0.511 +/- 0.553 for {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 250}\n",
      "0.578 +/- 0.579 for {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 5}\n",
      "0.451 +/- 0.584 for {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 50}\n",
      "0.442 +/- 0.577 for {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 250}\n",
      "0.478 +/- 0.604 for {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 5}\n",
      "0.436 +/- 0.586 for {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 50}\n",
      "0.451 +/- 0.607 for {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 250}\n",
      "0.694 +/- 0.569 for {'learning_rate': 1, 'max_depth': 1, 'n_estimators': 5}\n",
      "0.647 +/- 0.531 for {'learning_rate': 1, 'max_depth': 1, 'n_estimators': 50}\n",
      "0.622 +/- 0.512 for {'learning_rate': 1, 'max_depth': 1, 'n_estimators': 250}\n",
      "0.56 +/- 0.515 for {'learning_rate': 1, 'max_depth': 3, 'n_estimators': 5}\n",
      "0.506 +/- 0.547 for {'learning_rate': 1, 'max_depth': 3, 'n_estimators': 50}\n",
      "0.481 +/- 0.578 for {'learning_rate': 1, 'max_depth': 3, 'n_estimators': 250}\n",
      "0.473 +/- 0.651 for {'learning_rate': 1, 'max_depth': 5, 'n_estimators': 5}\n",
      "0.427 +/- 0.557 for {'learning_rate': 1, 'max_depth': 5, 'n_estimators': 50}\n",
      "0.43 +/- 0.552 for {'learning_rate': 1, 'max_depth': 5, 'n_estimators': 250}\n",
      "0.462 +/- 0.586 for {'learning_rate': 1, 'max_depth': 7, 'n_estimators': 5}\n",
      "0.438 +/- 0.531 for {'learning_rate': 1, 'max_depth': 7, 'n_estimators': 50}\n",
      "0.443 +/- 0.532 for {'learning_rate': 1, 'max_depth': 7, 'n_estimators': 250}\n",
      "0.12 +/- 0.038 for {'learning_rate': 10, 'max_depth': 1, 'n_estimators': 5}\n",
      "0.12 +/- 0.038 for {'learning_rate': 10, 'max_depth': 1, 'n_estimators': 50}\n",
      "0.12 +/- 0.038 for {'learning_rate': 10, 'max_depth': 1, 'n_estimators': 250}\n",
      "0.112 +/- 0.047 for {'learning_rate': 10, 'max_depth': 3, 'n_estimators': 5}\n",
      "0.112 +/- 0.047 for {'learning_rate': 10, 'max_depth': 3, 'n_estimators': 50}\n",
      "0.112 +/- 0.047 for {'learning_rate': 10, 'max_depth': 3, 'n_estimators': 250}\n",
      "0.081 +/- 0.067 for {'learning_rate': 10, 'max_depth': 5, 'n_estimators': 5}\n",
      "0.081 +/- 0.067 for {'learning_rate': 10, 'max_depth': 5, 'n_estimators': 50}\n",
      "0.081 +/- 0.067 for {'learning_rate': 10, 'max_depth': 5, 'n_estimators': 250}\n",
      "0.15 +/- 0.164 for {'learning_rate': 10, 'max_depth': 7, 'n_estimators': 5}\n",
      "0.207 +/- 0.399 for {'learning_rate': 10, 'max_depth': 7, 'n_estimators': 50}\n",
      "0.21 +/- 0.395 for {'learning_rate': 10, 'max_depth': 7, 'n_estimators': 250}\n",
      "CPU times: user 52min 40s, sys: 31.6 s, total: 53min 11s\n",
      "Wall time: 57min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gb = GradientBoostingClassifier()\n",
    "parameters = {\"n_estimators\" : [5, 50, 250],\n",
    "             \"max_depth\": [1, 3, 5, 7],\n",
    "             \"learning_rate\": [0.01, 0.1, 1, 10]}\n",
    "\n",
    "gb_cv = GridSearchCV(gb, parameters, cv=5)\n",
    "gb_cv.fit(X, y.ravel())\n",
    "\n",
    "print_models_results(gb_cv)\n",
    "gb_best = gb_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params = {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 5}\n",
    "gb_best = GradientBoostingClassifier(**best_params)\n",
    "gb_best.fit(X_train, y_train)\n",
    "y_pred = gb_best.predict(X_val)\n",
    "precision_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2381, 742)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred[y_pred==1]), len(y_val[y_val==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RF</th>\n",
       "      <td>0.112595</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GB</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   precision recall f1_score\n",
       "RF  0.112595      1   0.2024\n",
       "GB         0      0        0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_names = [\"RF\", \"GB\"]\n",
    "results_df = pd.DataFrame(index= model_names, columns = [\"precision\", \"recall\", \"f1_score\"])\n",
    "models = [rf_best, gb_best]\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    y_pred = models[i].predict(X_val)\n",
    "    results_df.loc[model_names[i], \"precision\"] = precision_score(y_val, y_pred)\n",
    "    results_df.loc[model_names[i], \"recall\"] = recall_score(y_val, y_pred)\n",
    "    results_df.loc[model_names[i], \"f1_score\"] = f1_score(y_val, y_pred)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance_RF</th>\n",
       "      <th>importance_GB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>0.023383</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>campaign</th>\n",
       "      <td>0.001970</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pdays</th>\n",
       "      <td>0.193934</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emp.var.rate</th>\n",
       "      <td>0.015304</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cons.price.idx</th>\n",
       "      <td>0.005993</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>euribor3m</th>\n",
       "      <td>0.158485</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nr.employed</th>\n",
       "      <td>0.353323</td>\n",
       "      <td>0.802532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poutcome_nonexistent</th>\n",
       "      <td>0.067245</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poutcome_success</th>\n",
       "      <td>0.180363</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      importance_RF  importance_GB\n",
       "age                        0.023383       0.000000\n",
       "campaign                   0.001970       0.000000\n",
       "pdays                      0.193934       0.000000\n",
       "emp.var.rate               0.015304       0.000000\n",
       "cons.price.idx             0.005993       0.000000\n",
       "euribor3m                  0.158485       0.000000\n",
       "nr.employed                0.353323       0.802532\n",
       "poutcome_nonexistent       0.067245       0.000000\n",
       "poutcome_success           0.180363       0.000000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances = pd.DataFrame(rf_best.feature_importances_, \n",
    "                                   index = df1.drop(\"y\", axis=1).columns, \n",
    "                                   columns=['importance_RF'])\n",
    "\n",
    "feature_importances[\"importance_GB\"] = gb_best.feature_importances_\n",
    "feature_importances[feature_importances[\"importance_RF\"]!=0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very bad performance of both models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2. Without Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((26360, 57), (26360,))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify= y)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, stratify= y_train)\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.fit_transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "splits = StratifiedShuffleSplit(n_splits=5, test_size=0.5, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': None, 'n_estimators': 250}\n",
      "\n",
      "0.891 +/- 0.01 for {'max_depth': 2, 'n_estimators': 5}\n",
      "0.892 +/- 0.007 for {'max_depth': 2, 'n_estimators': 50}\n",
      "0.889 +/- 0.002 for {'max_depth': 2, 'n_estimators': 250}\n",
      "0.901 +/- 0.006 for {'max_depth': 4, 'n_estimators': 5}\n",
      "0.901 +/- 0.002 for {'max_depth': 4, 'n_estimators': 50}\n",
      "0.901 +/- 0.003 for {'max_depth': 4, 'n_estimators': 250}\n",
      "0.905 +/- 0.005 for {'max_depth': 8, 'n_estimators': 5}\n",
      "0.907 +/- 0.003 for {'max_depth': 8, 'n_estimators': 50}\n",
      "0.907 +/- 0.003 for {'max_depth': 8, 'n_estimators': 250}\n",
      "0.903 +/- 0.002 for {'max_depth': 16, 'n_estimators': 5}\n",
      "0.911 +/- 0.002 for {'max_depth': 16, 'n_estimators': 50}\n",
      "0.912 +/- 0.002 for {'max_depth': 16, 'n_estimators': 250}\n",
      "0.902 +/- 0.003 for {'max_depth': None, 'n_estimators': 5}\n",
      "0.911 +/- 0.003 for {'max_depth': None, 'n_estimators': 50}\n",
      "0.912 +/- 0.003 for {'max_depth': None, 'n_estimators': 250}\n",
      "CPU times: user 2min 21s, sys: 3.16 s, total: 2min 24s\n",
      "Wall time: 2min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rf2 = RandomForestClassifier()\n",
    "parameters = {\"n_estimators\" : [5, 50, 250],\n",
    "             \"max_depth\": [2,4,8,16, None]}\n",
    "\n",
    "\n",
    "rf_cv2 = GridSearchCV(rf2, parameters, cv=splits) \n",
    "rf_cv2.fit(X, y.ravel())\n",
    "\n",
    "print_models_results(rf_cv2)\n",
    "rf_best2 = rf_cv2.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 50}\n",
      "\n",
      "0.887 +/- 0.0 for {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 5}\n",
      "0.887 +/- 0.0 for {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 50}\n",
      "0.895 +/- 0.002 for {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 250}\n",
      "0.887 +/- 0.0 for {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 5}\n",
      "0.887 +/- 0.0 for {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50}\n",
      "0.913 +/- 0.002 for {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 250}\n",
      "0.887 +/- 0.0 for {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 5}\n",
      "0.887 +/- 0.0 for {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 50}\n",
      "0.916 +/- 0.002 for {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 250}\n",
      "0.887 +/- 0.0 for {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 5}\n",
      "0.887 +/- 0.0 for {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 50}\n",
      "0.914 +/- 0.002 for {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 250}\n",
      "0.887 +/- 0.0 for {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 5}\n",
      "0.906 +/- 0.002 for {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 50}\n",
      "0.911 +/- 0.003 for {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 250}\n",
      "0.887 +/- 0.0 for {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 5}\n",
      "0.916 +/- 0.002 for {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50}\n",
      "0.916 +/- 0.002 for {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 250}\n",
      "0.887 +/- 0.0 for {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 5}\n",
      "0.917 +/- 0.002 for {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 50}\n",
      "0.915 +/- 0.002 for {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 250}\n",
      "0.888 +/- 0.001 for {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 5}\n",
      "0.914 +/- 0.002 for {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 50}\n",
      "0.91 +/- 0.002 for {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 250}\n",
      "0.905 +/- 0.002 for {'learning_rate': 1, 'max_depth': 1, 'n_estimators': 5}\n",
      "0.911 +/- 0.003 for {'learning_rate': 1, 'max_depth': 1, 'n_estimators': 50}\n",
      "0.911 +/- 0.003 for {'learning_rate': 1, 'max_depth': 1, 'n_estimators': 250}\n",
      "0.909 +/- 0.007 for {'learning_rate': 1, 'max_depth': 3, 'n_estimators': 5}\n",
      "0.911 +/- 0.006 for {'learning_rate': 1, 'max_depth': 3, 'n_estimators': 50}\n",
      "0.911 +/- 0.005 for {'learning_rate': 1, 'max_depth': 3, 'n_estimators': 250}\n",
      "0.907 +/- 0.007 for {'learning_rate': 1, 'max_depth': 5, 'n_estimators': 5}\n",
      "0.899 +/- 0.004 for {'learning_rate': 1, 'max_depth': 5, 'n_estimators': 50}\n",
      "0.897 +/- 0.005 for {'learning_rate': 1, 'max_depth': 5, 'n_estimators': 250}\n",
      "0.904 +/- 0.002 for {'learning_rate': 1, 'max_depth': 7, 'n_estimators': 5}\n",
      "0.897 +/- 0.005 for {'learning_rate': 1, 'max_depth': 7, 'n_estimators': 50}\n",
      "0.898 +/- 0.009 for {'learning_rate': 1, 'max_depth': 7, 'n_estimators': 250}\n",
      "0.125 +/- 0.002 for {'learning_rate': 10, 'max_depth': 1, 'n_estimators': 5}\n",
      "0.125 +/- 0.002 for {'learning_rate': 10, 'max_depth': 1, 'n_estimators': 50}\n",
      "0.125 +/- 0.002 for {'learning_rate': 10, 'max_depth': 1, 'n_estimators': 250}\n",
      "0.138 +/- 0.063 for {'learning_rate': 10, 'max_depth': 3, 'n_estimators': 5}\n",
      "0.138 +/- 0.063 for {'learning_rate': 10, 'max_depth': 3, 'n_estimators': 50}\n",
      "0.138 +/- 0.063 for {'learning_rate': 10, 'max_depth': 3, 'n_estimators': 250}\n",
      "0.135 +/- 0.02 for {'learning_rate': 10, 'max_depth': 5, 'n_estimators': 5}\n",
      "0.135 +/- 0.02 for {'learning_rate': 10, 'max_depth': 5, 'n_estimators': 50}\n",
      "0.135 +/- 0.02 for {'learning_rate': 10, 'max_depth': 5, 'n_estimators': 250}\n",
      "0.148 +/- 0.023 for {'learning_rate': 10, 'max_depth': 7, 'n_estimators': 5}\n",
      "0.141 +/- 0.013 for {'learning_rate': 10, 'max_depth': 7, 'n_estimators': 50}\n",
      "0.146 +/- 0.021 for {'learning_rate': 10, 'max_depth': 7, 'n_estimators': 250}\n"
     ]
    }
   ],
   "source": [
    "gb2 = GradientBoostingClassifier()\n",
    "parameters = {\"n_estimators\" : [5, 50, 250],\n",
    "             \"max_depth\": [1, 3, 5, 7],\n",
    "             \"learning_rate\": [0.01, 0.1, 1, 10]}\n",
    "\n",
    "gb_cv_2 = GridSearchCV(gb2, parameters, cv=splits)\n",
    "gb_cv_2.fit(X, y.ravel())\n",
    "\n",
    "print_models_results(gb_cv_2)\n",
    "gb_best2 = gb_cv_2.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RF</th>\n",
       "      <td>0.885025</td>\n",
       "      <td>0.5192</td>\n",
       "      <td>0.50852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GB</th>\n",
       "      <td>0.567701</td>\n",
       "      <td>0.65636</td>\n",
       "      <td>0.543643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   precision   recall  f1_score\n",
       "RF  0.885025   0.5192   0.50852\n",
       "GB  0.567701  0.65636  0.543643"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_names = [\"RF\", \"GB\"]\n",
    "results_df = pd.DataFrame(index= model_names, columns = [\"precision\", \"recall\", \"f1_score\"])\n",
    "models = [rf_best2, gb_best2]\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    y_pred = models[i].predict(X_val)\n",
    "    results_df.loc[model_names[i], \"precision\"] = precision_score(y_val, y_pred, average = \"macro\")\n",
    "    results_df.loc[model_names[i], \"recall\"] = recall_score(y_val, y_pred, average = \"macro\")\n",
    "    results_df.loc[model_names[i], \"f1_score\"] = f1_score(y_val, y_pred, average = \"macro\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance_RF</th>\n",
       "      <th>importance_GB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nr.employed</th>\n",
       "      <td>0.353323</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             importance_RF  importance_GB\n",
       "nr.employed       0.353323            1.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances = pd.DataFrame(rf_best.feature_importances_, \n",
    "                                   index = df1.drop(\"y\", axis=1).columns, \n",
    "                                   columns=['importance_RF'])\n",
    "\n",
    "feature_importances[\"importance_GB\"] = gb_best.feature_importances_\n",
    "feature_importances[feature_importances[\"importance_GB\"]!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myspark",
   "language": "python",
   "name": "myspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
